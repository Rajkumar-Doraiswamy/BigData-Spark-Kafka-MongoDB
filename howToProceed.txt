# Last amended: 12th July, 2019
# Myfolder:  /home/ashok/Documents/spark/streaming/3.kafka_spark_streaming

# Objectives:
#            1. Feed streams from kafka to spark and perform analysis
#		i)  WordCount
#		ii) Online Realtime Sentiment Analysis



# 0. Start hadoop

       ./allstart.sh

#1.
# Files in this folder are examples of kafka-streaming and spark-streaming.
# Streams from kafka are received in spark. Spark-streaming then analyses
# these streams.

2. Files
	2.1 Data files are in folder: 
                /home/ashok/Documents/spark/streaming/3.kafka_spark_streaming/data

	2.2 Sparkstreaming python files are in folder
		/home/ashok/Documents/spark/streaming/3.kafka_spark_streaming/python_files

	2.3 Kafka files are in this folder:
		/home/ashok/Documents/spark/streaming/3.kafka_spark_streaming

Downloads:
https://mvnrepository.com/artifact/org.apache.spark/spark-streaming-kafka-0-10-assembly_2.11/2.4.3

3.
Experiment 1 (Word_count):
=============================

	3.1 Prepare

cd ~
rm -f /opt/kafka_2.11-1.0.1/logs/*
rm -f /home/kafka/confluent-4.0.0/log/*logs*  
rm -rf /tmp/zookeeper
rm -rf /tmp/kafka-logs*
rm -rf /tmp/confluent*
rm -rf /tmp/ksql*
rm  ~/useless.txt


3.1. Start Kafka
============

	3.1.1	Start zookeeper:

cd ~
bash  /home/ashok/Documents/spark/streaming/kafka_spark_streaming/1.startZookeeper.sh

			(Note: zookeeper displays INFO level errors. Do not worry.)

	3.1.2 	Starts kafka broker:

cd ~
bash  /home/ashok/Documents/spark/streaming/kafka_spark_streaming/2.startKafkaBroker.sh

	3.1.3	Creates a topic 'test':

cd ~
bash  /home/ashok/Documents/spark/streaming/kafka_spark_streaming/3.createTopic.sh

	3.1.4	List created topics:

cd ~
bash  /home/ashok/Documents/spark/streaming/kafka_spark_streaming/4.listTopic.sh


4.0 Analyse stream using spark (at this moment there are no streams):

	4.1	Generate a kafka consumer using spark. It also analyses data (word-count).
                 The analysed data is dumped into file:  'useless.txt'

cd ~
bash /home/ashok/Documents/spark/streaming/kafka_spark_streaming/5.spark_stream_consume.sh

	4.2 	Create a stream through kafka producer. The producer gets a 
		 redirected file of car.data

cd ~
bash  /home/ashok/Documents/spark/streaming/kafka_spark_streaming/6.kafka_produce_frm_file.sh

	4.3 You can also experiment with generating streams from console ie console producer. The kafka
                command is:

 kafka-console-producer.sh --broker-list localhost:9092 --topic test

	4.4.  See file useless.txt for streaming results:
		
cd ~
cat  ~/useless.txt	


	8. Kill all processes, as: ctrl+c
		And one process as:
			ps aux | grep zookeeper
			kill -9 4143


5.
Experiment 2 (Sentiment analysis):
======================================

Kill all above processes

	5.1. Prepare afresh:

# A

cd ~
rm -f /opt/kafka_2.11-1.0.1/logs/*
rm -rf /tmp/zookeeper
rm -rf /tmp/kafka-logs*
rm -rf /tmp/confluent*
rm -rf /tmp/confluent*
rm -rf /tmp/ksql*
rm  ~/useless.txt


# B 
# 5.2 Transfer requisite file, AFINN-111.txt,  to hdfs. This file gives +ve, -ve scores to words.


hdfs dfs -put /home/ashok/Documents/spark/streaming/kafka_spark_streaming/data/AFINN-111.txt   /user/ashok/data_files/


	5.3 	Starts zookeeper

cd ~
bash /home/ashok/Documents/spark/streaming/kafka_spark_streaming/1.startZookeeper.sh

			(Note: zookeeper displays INFO level errors. Do not worry.)

	5.4.	Starts kafka broker

cd ~
bash  /home/ashok/Documents/spark/streaming/kafka_spark_streaming/2.startKafkaBroker.sh

	5.5.	Creates a topic test

cd ~
bash  /home/ashok/Documents/spark/streaming/kafka_spark_streaming/3.createTopic.sh

	5.6.	List created topics

cd ~
bash  /home/ashok/Documents/spark/streaming/kafka_spark_streaming/4.listTopic.sh


	5.7.	Spark Sentiment Consumer: Generates a kafka consumer using spark. It also
					  analyses data (simple sentiment_analysys). But at this time there is no input stream
					  to analyse:

cd ~
bash  /home/ashok/Documents/spark/streaming/kafka_spark_streaming/7.spark_stream_consume_sentiment.sh


	5.8 Two producer feeds:
                5.8.1 Console Producer Feed: Creates a kafka producer that takes inputs from console: 

cd ~
bash  /home/ashok/Documents/spark/streaming/kafka_spark_streaming/9.kafka_produce_frm_console.sh

		Feed sentences, as:

                       Very good work
                       Very disappointing work
                       Too bad
                       Take advantage of time
                       Adorable dog
                       Hate holidays


	    5.8.2   File Producer Feed: Creates a kafka producer. The producer gets a 
		          redirected file of happy_sad.data

			$ cd ~
			$ bash  /home/ashok/Documents/spark/streaming/kafka_spark_streaming/8.kafka_produce_frm_file_happy_sad.sh


       5.9   Open the file ~/useless.txt to see results

                 leafpad   /home/kafka/useless.txt


	6. Kill all processes


#####################################
# Ref: https://community.hortonworks.com/questions/42793/kafka-producer-error-no-partition-metadata-for-top.html
# Kill a topic 'test'
kafka-topics.sh --zookeeper localhost:2181 --delete --topic test


