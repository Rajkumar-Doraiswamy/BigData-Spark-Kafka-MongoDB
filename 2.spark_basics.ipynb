{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Last amended: 15/08/2019\n",
    "#  Myfolder: /home/ashok/Documents/spark\n",
    "# Ref: https://mingchen0919.github.io/learning-apache-spark/categorical-data.html\n",
    "#      https://www.analyticsvidhya.com/blog/2016/10/spark-dataframe-and-operations/\n",
    "\n",
    "\n",
    "# Objectives:\n",
    "#            1. Dealing with categorical columns\n",
    "#            2. Using StingIndexer, OneHotEncoderEstimator, VectorAssember"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A. Create some data\n",
    "\n",
    "# This data frame will be used to demonstrate how to use \n",
    "#                  a) StingIndexer,\n",
    "#                  b) OneHotEncoderEstimator, \n",
    "#                  c) VectorAssember\n",
    "\n",
    "# x1 and x2 are categorical columns type strings.\n",
    "# x3 is a categorical column with integers.\n",
    "# x4 is a numerical column. \n",
    "# y1 is a categorical column type integer.\n",
    "# y2 is a column of type string. \n",
    "\n",
    "# 1.0\n",
    "import pandas as pd\n",
    "\n",
    "# 1.1\n",
    "pdf = pd.DataFrame({\n",
    "                    'x1': ['a','a','b','b', 'b', 'c', 'd','d'],\n",
    "                    'x2': ['apple', 'orange', 'orange','orange', 'peach', 'peach','apple','orange'],\n",
    "                    'x3': [1, 1, 2, 2, 2, 4, 1, 2],\n",
    "                    'x4': [2.4, 2.5, 3.5, 1.4, 2.1,1.5, 3.0, 2.0],\n",
    "                    'y1': [1, 0, 1, 0, 0, 1, 1, 0],\n",
    "                    'y2': ['yes', 'no', 'no', 'yes', 'yes', 'yes', 'no', 'yes']\n",
    "                   })  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.2\n",
    "df = spark.createDataFrame(pdf)\n",
    "type(df)           # pyspark.sql.dataframe.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+---+---+\n",
      "| x1|    x2| x3| x4| y1| y2|\n",
      "+---+------+---+---+---+---+\n",
      "|  a| apple|  1|2.4|  1|yes|\n",
      "|  a|orange|  1|2.5|  0| no|\n",
      "|  b|orange|  2|3.5|  1| no|\n",
      "+---+------+---+---+---+---+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+----+-----+------------------+------------------+------------------+----+\n",
      "|summary|  x1|   x2|                x3|                x4|                y1|  y2|\n",
      "+-------+----+-----+------------------+------------------+------------------+----+\n",
      "|  count|   8|    8|                 8|                 8|                 8|   8|\n",
      "|   mean|null| null|             1.875|               2.3|               0.5|null|\n",
      "| stddev|null| null|0.9910312089651149|0.7131419413913535|0.5345224838248488|null|\n",
      "|    min|   a|apple|                 1|               1.4|                 0|  no|\n",
      "|    max|   d|peach|                 4|               3.5|                 1| yes|\n",
      "+-------+----+-----+------------------+------------------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# B. About DataFrame\n",
    "# Ref: https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_SQL_Cheat_Sheet_Python.pdf\n",
    "# 2.0\n",
    "df.show(3)          # Show data\n",
    "df.head(3)\n",
    "df.take(2)         # Show two rows\n",
    "type(df.take(2))   # List of objects: pyspark.sql.types.Row\n",
    "r = df.take(2)\n",
    "r[0]               # First row\n",
    "type(r[0])         # pyspark.sql.types.Row\n",
    "df.describe().show()  # Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. StringIndexer\n",
    "\n",
    "StringIndexer maps a string column to a index column that will be treated as a categorical column by spark. <br>\n",
    "The indices start with 0 and are ordered by label frequencies. If it is a numerical column, the column will first<br>\n",
    "be casted to a string column and then indexed by StringIndexer.<br>\n",
    "There are three steps to implement the StringIndexer<br>\n",
    "-      Build the StringIndexer model: specify the input column and output column names.\n",
    "-      Learn the StringIndexer model: fit the model with your data.\n",
    "-      Execute the indexing: call the transform function to execute the indexing process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1\n",
    "# build indexer. No need to specify dataframe here, just column names\n",
    "#                               inputCol and outputCol are not lists:\n",
    "\n",
    "string_indexer = StringIndexer(inputCol='x1',\n",
    "                               outputCol='indexed_x1'\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Learn/fit the model on dataframe:\n",
    "\n",
    "si_model = string_indexer.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Transform the data to a new DataFrame:\n",
    "\n",
    "df_si = si_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+---+---+----------+\n",
      "| x1|    x2| x3| x4| y1| y2|indexed_x1|\n",
      "+---+------+---+---+---+---+----------+\n",
      "|  a| apple|  1|2.4|  1|yes|       2.0|\n",
      "|  a|orange|  1|2.5|  0| no|       2.0|\n",
      "|  b|orange|  2|3.5|  1| no|       0.0|\n",
      "|  b|orange|  2|1.4|  0|yes|       0.0|\n",
      "|  b| peach|  2|2.1|  0|yes|       0.0|\n",
      "|  c| peach|  4|1.5|  1|yes|       3.0|\n",
      "|  d| apple|  1|3.0|  1| no|       1.0|\n",
      "|  d|orange|  2|2.0|  0|yes|       1.0|\n",
      "+---+------+---+---+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3.4 Resulting df\n",
    "#     From the result it can be seen that (a, b, c) in column x1 are converted to\n",
    "#     (1.0, 0.0, 2.0). They are ordered by their frequencies in column x1.\n",
    "#     Max freq value is coded as 0.\n",
    "\n",
    "df_si.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. OneHotEncoderEstimator\n",
    "\n",
    "One-hot encoding maps a categorical feature, represented as a label index,<br>\n",
    "to a binary vector with at most a single one-value indicating the presence of<br>\n",
    "a specific feature value from among the set of all feature values. This encoding<br>\n",
    "allows algorithms which expect continuous features, such as Logistic Regression,<br>\n",
    "to use categorical features. For string type input data, it is common to encode<br>\n",
    "categorical features using StringIndexer first.<br>\n",
    "OneHotEncoderEstimator can transform multiple columns, returning an <br>\n",
    "one-hot-encoded output vector column for each input column. It is common to<br>\n",
    "merge these vectors into a single feature vector using VectorAssembler.<br>\n",
    "\n",
    "\n",
    "Each index is converted to a vector. However, in spark the vector is represented by a<br>\n",
    "sparse vector, becase sparse vector can save a lot of memory.<br>\n",
    "The process of using OneHotEncoder is different to using StingIndexer. <br>\n",
    "There are only two steps.<br>\n",
    "-    i) Build an indexer model\n",
    "-    ii) Execute the indexing by calling transform\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.0\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Build OHEE.    Only specify the input/output columns.:\n",
    "#                    Multiple columns can be specified:\n",
    "\n",
    "onehotencoder = OneHotEncoderEstimator(\n",
    "                                       inputCols= ['indexed_x1'],\n",
    "                                       outputCols=['onehotencoded_x1']\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Transform df_si DataFrame to df_dummy\n",
    "\n",
    "model = onehotencoder.fit(df_si)\n",
    "df_dummy = model.transform(df_si)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+---+---+---+----------+----------------+\n",
      "| x1|    x2| x3| x4| y1| y2|indexed_x1|onehotencoded_x1|\n",
      "+---+------+---+---+---+---+----------+----------------+\n",
      "|  a| apple|  1|2.4|  1|yes|       2.0|   (3,[2],[1.0])|\n",
      "|  a|orange|  1|2.5|  0| no|       2.0|   (3,[2],[1.0])|\n",
      "|  b|orange|  2|3.5|  1| no|       0.0|   (3,[0],[1.0])|\n",
      "|  b|orange|  2|1.4|  0|yes|       0.0|   (3,[0],[1.0])|\n",
      "|  b| peach|  2|2.1|  0|yes|       0.0|   (3,[0],[1.0])|\n",
      "|  c| peach|  4|1.5|  1|yes|       3.0|       (3,[],[])|\n",
      "|  d| apple|  1|3.0|  1| no|       1.0|   (3,[1],[1.0])|\n",
      "|  d|orange|  2|2.0|  0|yes|       1.0|   (3,[1],[1.0])|\n",
      "+---+------+---+---+---+---+----------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.3 Resulting df\n",
    "# (3,[2],[1.0])  => Vector length: 3, At second   position, value is 1\t=   0 1 0 0\n",
    "# (3,[0],[1.0])  => Vector length: 3, At 0th      position, value is 1\t=  1 0 0 0\t\n",
    "#  (3,[],[])\t => Vector length: 3  At 3rd/last position, value is 1\t=  0 0 0 1\t\n",
    "\n",
    "df_dummy.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## E. Process all categorical columns with Pipeline\n",
    "#     A Pipeline is a sequence of stages. A stage is an instance which has the property of either fit()\n",
    "#      or transform(). When fitting a Pipeline, the stages get executed in order. The example below shows\n",
    "#       how to use pipeline to process all categorical columns.\n",
    "\n",
    "# 5. List all categorical columns\n",
    "categorical_columns = ['x1', 'x2', 'x3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##=== build stages ======\n",
    "# 5.1\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='stringindexed_' + c) for c in categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_7afc5a94c664,\n",
       " StringIndexer_23ec52d81953,\n",
       " StringIndexer_df2f7af6bd99]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stringindexer_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2\n",
    "in_cols = ['stringindexed_' + c for c in categorical_columns]\n",
    "out_cols = ['onehotencoded_' + c  for c in categorical_columns]\n",
    "onehotencoder_stages = [OneHotEncoderEstimator(inputCols=in_cols, outputCols=out_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.3\n",
    "all_stages = stringindexer_stages + onehotencoder_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StringIndexer_7afc5a94c664,\n",
       " StringIndexer_23ec52d81953,\n",
       " StringIndexer_df2f7af6bd99,\n",
       " OneHotEncoderEstimator_2128f5d440d2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.4 Build pipeline model\n",
    "# 5.4.1\n",
    "\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.4.2\n",
    "\n",
    "pipeline = Pipeline(stages=all_stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 5.5 Fit pipeline model\n",
    "\n",
    "pipeline_mode = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x1',\n",
       " 'x2',\n",
       " 'x3',\n",
       " 'x4',\n",
       " 'y1',\n",
       " 'y2',\n",
       " 'stringindexed_x1',\n",
       " 'stringindexed_x2',\n",
       " 'stringindexed_x3',\n",
       " 'onehotencoded_x1',\n",
       " 'onehotencoded_x2',\n",
       " 'onehotencoded_x3']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 5.6 Transform data\n",
    "\n",
    "df_coded = pipeline_mode.transform(df)\n",
    "df_coded.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+----------------+---+---+---+\n",
      "|onehotencoded_x1|onehotencoded_x2|onehotencoded_x3| x4| y1| y2|\n",
      "+----------------+----------------+----------------+---+---+---+\n",
      "|   (3,[2],[1.0])|       (2,[],[])|   (2,[1],[1.0])|2.4|  1|yes|\n",
      "|   (3,[2],[1.0])|   (2,[0],[1.0])|   (2,[1],[1.0])|2.5|  0| no|\n",
      "|   (3,[0],[1.0])|   (2,[0],[1.0])|   (2,[0],[1.0])|3.5|  1| no|\n",
      "|   (3,[0],[1.0])|   (2,[0],[1.0])|   (2,[0],[1.0])|1.4|  0|yes|\n",
      "|   (3,[0],[1.0])|   (2,[1],[1.0])|   (2,[0],[1.0])|2.1|  0|yes|\n",
      "|       (3,[],[])|   (2,[1],[1.0])|       (2,[],[])|1.5|  1|yes|\n",
      "|   (3,[1],[1.0])|       (2,[],[])|   (2,[1],[1.0])|3.0|  1| no|\n",
      "|   (3,[1],[1.0])|   (2,[0],[1.0])|   (2,[0],[1.0])|2.0|  0|yes|\n",
      "+----------------+----------------+----------------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 6. Remove uncoded columns\n",
    "\n",
    "selected_columns = ['onehotencoded_' + c for c in categorical_columns] + ['x4', 'y1', 'y2']\n",
    "df_coded = df_coded.select(selected_columns)\n",
    "df_coded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+---+---+---+---+-------+-------+-------+-------------+---------+-------------+\n",
      "| x1|   x2| x3| x4| y1| y2|x1index|x2index|x3index|      x1trans|  x2trans|      x3trans|\n",
      "+---+-----+---+---+---+---+-------+-------+-------+-------------+---------+-------------+\n",
      "|  a|apple|  1|2.4|  1|yes|    2.0|    2.0|    1.0|(3,[2],[1.0])|(2,[],[])|(2,[1],[1.0])|\n",
      "+---+-----+---+---+---+---+-------+-------+-------+-------------+---------+-------------+\n",
      "only showing top 1 row\n",
      "\n",
      "+-------------+-------------+-------------+---+---+---+\n",
      "|      x1trans|      x2trans|      x3trans| x4| y1| y2|\n",
      "+-------------+-------------+-------------+---+---+---+\n",
      "|(3,[2],[1.0])|    (2,[],[])|(2,[1],[1.0])|2.4|  1|yes|\n",
      "|(3,[2],[1.0])|(2,[0],[1.0])|(2,[1],[1.0])|2.5|  0| no|\n",
      "|(3,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|3.5|  1| no|\n",
      "|(3,[0],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|1.4|  0|yes|\n",
      "|(3,[0],[1.0])|(2,[1],[1.0])|(2,[0],[1.0])|2.1|  0|yes|\n",
      "|    (3,[],[])|(2,[1],[1.0])|    (2,[],[])|1.5|  1|yes|\n",
      "|(3,[1],[1.0])|    (2,[],[])|(2,[1],[1.0])|3.0|  1| no|\n",
      "|(3,[1],[1.0])|(2,[0],[1.0])|(2,[0],[1.0])|2.0|  0|yes|\n",
      "+-------------+-------------+-------------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "# 7 The above pipline code is equivalent to following:\n",
    "\n",
    "cat_cols = ['x1', 'x2', 'x3']\n",
    "stages = []\n",
    "in_cols = []\n",
    "out_cols = []\n",
    "\n",
    "for i in cat_cols:\n",
    "    si = StringIndexer(inputCol=i, outputCol = i + \"index\")\n",
    "    in_cols.append(i+\"index\")\n",
    "    out_cols.append(i+\"trans\")\n",
    "    stages.append(si)\n",
    "\n",
    "ohe = OneHotEncoderEstimator(inputCols=in_cols, outputCols=out_cols)\n",
    "stages.append(ohe)\n",
    "stages\n",
    "pipeline = Pipeline(stages=stages)\n",
    "pipeline_mode = pipeline.fit(df)\n",
    "df_coded1 = pipeline_mode.transform(df)\n",
    "df_coded1.columns\n",
    "df_coded1.show(1)\n",
    "\n",
    "selected_columns = [i+\"trans\" for i in cat_cols] + ['x4', 'y1', 'y2']\n",
    "rest = df_coded1.select(selected_columns)\n",
    "rest.show()\n",
    "################################################\n",
    " \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
